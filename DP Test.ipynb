{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from math import isclose\n",
    "\n",
    "from frozendict import frozendict\n",
    "from random import choice, choices\n",
    "\n",
    "\n",
    "class State(frozendict):\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        return frozendict.__new__(cls, *args, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        if 'title' in self:\n",
    "            return self['title']\n",
    "\n",
    "        return f\"L{self['level']} ({'T' if self['tired'] else 'R'})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, states, statesPlus, actions, transitions=None, rewards=None, gamma=0.9, eps=1e-6, T=100, costOfLiving=0,\n",
    "                 startingState=None):\n",
    "        self.states = states\n",
    "        self.statesPlus = statesPlus\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.T = T\n",
    "        self.costOfLiving = costOfLiving\n",
    "\n",
    "        self.startingState = startingState if startingState else states[0]\n",
    "        self.currentState = self.startingState\n",
    "        self.stepsTaken = 0\n",
    "\n",
    "        self.checkProbabilities()\n",
    "\n",
    "    def checkProbabilities(self):\n",
    "        for s, a in self.transitions.items():\n",
    "            for action, outcomes in a.items():\n",
    "                try:\n",
    "                    assert isclose(sum(outcomes.values()), 1,\n",
    "                                   abs_tol=1e-4)  # Making sure the sum of outcomes is effectively 1\n",
    "                except AssertionError:\n",
    "                    raise AssertionError(f\"Probability not 1: {s} + {action} -> {outcomes.values()}\")\n",
    "\n",
    "                for o, chance in outcomes.items():\n",
    "                    try:\n",
    "                        assert 0 <= chance <= 1\n",
    "                    except AssertionError:\n",
    "                        raise AssertionError(f\"Invalid probability 1: {s} + {action} -> {o} ({chance})\")\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.currentState = self.startingState\n",
    "        self.stepsTaken = 0\n",
    "\n",
    "        return self.currentState\n",
    "\n",
    "    @property\n",
    "    def stateSpace(self) -> int:\n",
    "        return len(self.states)\n",
    "\n",
    "    @property\n",
    "    def allActions(self) -> set:\n",
    "        return set([a for A in self.actions.values() for a in A])\n",
    "\n",
    "    @property\n",
    "    def actionSpace(self) -> int:\n",
    "        return len(self.allActions)  # To set to remove duplicate actions\n",
    "\n",
    "    def getActions(self) -> list:\n",
    "        return self.actions[self.currentState]\n",
    "\n",
    "    def isTerminal(self, state):\n",
    "        return state not in self.transitions\n",
    "\n",
    "    def getReward(self, state, action, newState):\n",
    "        return self.rewards.get(state, {}).get(action, {}).get(newState, 0) + self.costOfLiving\n",
    "\n",
    "    def step(self, action, message=False):\n",
    "        if action not in self.actions[self.currentState]:\n",
    "            print('\\tInvalid action:', action)\n",
    "            return (self.currentState,\n",
    "                    0,\n",
    "                    False,\n",
    "                    False,\n",
    "                    \"Invalid\")\n",
    "\n",
    "        possibleOutcomes = self.transitions[self.currentState][action]  # {s: chance, s: chance, etc.}\n",
    "        newState = choices(list(possibleOutcomes.keys()), list(possibleOutcomes.values()))[0]\n",
    "        reward = self.getReward(self.currentState, action, newState)\n",
    "\n",
    "        if message:\n",
    "            print(f\"\\t{self.currentState} -> {newState} via {action} (reward of {reward:.2f})\")\n",
    "\n",
    "        self.stepsTaken += 1\n",
    "        self.currentState = newState\n",
    "\n",
    "        return (newState,\n",
    "                reward,\n",
    "                self.isTerminal(newState),\n",
    "                # In our case we treat 'sinks' (states with no further transitions) as terminal.\n",
    "                self.stepsTaken >= self.T,\n",
    "                None)  # What should info be?\n",
    "\n",
    "\n",
    "def Test(mdp: MDP, how: str, maxLength: int) -> None:\n",
    "    state = mdp.reset()  # reset/re-initialize\n",
    "    totalRewards = 0\n",
    "\n",
    "    print(f\"Testing MDP with action '{how}' for {maxLength} turns.\")\n",
    "    for _ in range(maxLength):\n",
    "        action = choice(mdp.getActions()) if how == 'random' else how\n",
    "\n",
    "        newState, reward, terminated, truncated, info = mdp.step(\n",
    "            action)  # execute an action in the current state of MDP\n",
    "\n",
    "        if info == 'Invalid':\n",
    "            return\n",
    "\n",
    "        totalRewards += reward\n",
    "        print(f\"\\t{state} -> {newState} via {action} (reward of {reward:.2f}, total of {totalRewards:.2f})\")\n",
    "\n",
    "        if terminated:\n",
    "            print('\\tDone!')\n",
    "            break\n",
    "\n",
    "        state = newState\n",
    "\n",
    "\n",
    "def ActionsFromTransitions(t: dict) -> dict:  # state: {action1: outcomes, action2: outcomes} -> state: [actions]\n",
    "    return {s: list(A) for s, A in t.items()}\n",
    "\n",
    "\n",
    "def StatesFromTransitions(t: dict) -> list:\n",
    "    return list(t)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84cbf1fdfb9eabd1"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "def ToTired(s: State) -> State:\n",
    "    return State(level=s['level'],\n",
    "                 tired=True)\n",
    "\n",
    "\n",
    "def ToRested(s: State) -> State:\n",
    "    return State(level=s['level'],\n",
    "                 tired=False)\n",
    "\n",
    "\n",
    "def ToTrained(s: State) -> State:\n",
    "    return State(level=s['level'] + 1,\n",
    "                 tired=True)\n",
    "\n",
    "\n",
    "def WinChance(s: State, maxLevel: int, invert: bool = False) -> float:\n",
    "    # winChance = 0.1 + (0.9 / (maxLevel - 1)) * (s['level'] - 1)\n",
    "    winChance = 0.2 * s['level'] - 0.1\n",
    "\n",
    "    if s['tired']:\n",
    "        winChance /= 2\n",
    "\n",
    "    return (1 - winChance) if invert else winChance\n",
    "\n",
    "\n",
    "def GenerateMDP(maxLevel: int, attackedChance: float, costOfLiving: float = 0, startingState: State = None) -> MDP:\n",
    "    states = [State(level=l, tired=t) for l in range(1, maxLevel + 1) for t in (True, False)]\n",
    "\n",
    "    won, died = State(title=\"Won\"), State(title=\"Died\")\n",
    "    statesPlus = states + [won, died]\n",
    "\n",
    "    transitions = {}\n",
    "\n",
    "    for s in states:\n",
    "        t = {'attack': {won: WinChance(s, maxLevel),\n",
    "                        died: WinChance(s, maxLevel, True)}}\n",
    "\n",
    "        if s['tired']:\n",
    "            t['rest'] = {ToRested(s): 1 - attackedChance,\n",
    "                         died: attackedChance}\n",
    "        else:\n",
    "            t['defend'] = {s: 1 - attackedChance,\n",
    "                           ToTired(s): attackedChance}\n",
    "\n",
    "            if s['level'] < maxLevel:\n",
    "                t['train'] = {ToTrained(s): 1 - attackedChance,\n",
    "                              died: attackedChance}\n",
    "\n",
    "        transitions[s] = t\n",
    "\n",
    "    rewards = {s: {'attack': {won: 1, died: -1},\n",
    "                   'defend': {died: -1},\n",
    "                   'train': {died: -1},\n",
    "                   'rest': {died: -1}}\n",
    "               for s in states}\n",
    "\n",
    "\n",
    "    return MDP(states,\n",
    "               statesPlus,\n",
    "               ActionsFromTransitions(transitions),\n",
    "               transitions=transitions,\n",
    "               rewards=rewards,\n",
    "               costOfLiving=costOfLiving,\n",
    "               startingState=states[0] if not startingState else startingState)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T12:52:37.095022500Z",
     "start_time": "2024-06-18T12:52:37.041023900Z"
    }
   },
   "id": "e4799956ef0edd9"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "GAMMA = 0.95\n",
    "\n",
    "\n",
    "def Gt(state: State, action: str, nextState: State, mdp: MDP, vTable: dict) -> float:\n",
    "    # print(f\"GT ({state}, {action} -> {nextState}): {mdp.getReward(state, action, nextState)}, s': {vTable[nextState]}\")\n",
    "    return mdp.getReward(state, action, nextState) + (GAMMA * vTable[nextState])\n",
    "\n",
    "\n",
    "# Bellman equation for V\n",
    "def CalculateV(policy: dict, state: State, mdp: MDP, vTable: dict) -> float:\n",
    "    if mdp.isTerminal(state):\n",
    "        return 0\n",
    "    \n",
    "    return sum(p * CalculateQ(state, a, mdp, vTable)\n",
    "               for a, p in policy[state].items())\n",
    "\n",
    "\n",
    "# Bellman equation for Q\n",
    "def CalculateQ(state: State, action: str, mdp: MDP, vTable: dict) -> float:\n",
    "    return sum(p * Gt(state, action, sNext, mdp, vTable)\n",
    "               for sNext, p in mdp.transitions[state][action].items())\n",
    "\n",
    "\n",
    "def UpdateQ(state: State, policy: dict, mdp: MDP, vTable: dict) -> dict:\n",
    "    return {a: CalculateQ(state, a, mdp, vTable) for a in policy[state]}\n",
    "\n",
    "\n",
    "def ArgMax(d: dict):  # np.argmax was causing issues.\n",
    "    for k in d:\n",
    "        if d[k] == max(d.values()):\n",
    "            return k\n",
    "\n",
    "\n",
    "def UpdatePolicy(policy: dict, actionValues: dict) -> dict:  # Choosing the optimal action from our actionValues.\n",
    "    updatedPolicy = deepcopy(policy)\n",
    "\n",
    "    for state in policy:\n",
    "        optimalAction = ArgMax(actionValues[state])\n",
    "\n",
    "        updatedPolicy[state] = {a: int(a == optimalAction) for a in policy[state]}\n",
    "\n",
    "    return updatedPolicy\n",
    "\n",
    "\n",
    "def UpdateV(vTable: dict, policy: dict, mdp: MDP, sweeps: int) -> dict:\n",
    "    for _ in range(sweeps):\n",
    "        tableCopy = deepcopy(vTable)\n",
    "        \n",
    "        vTable = {s: CalculateV(policy, s, mdp, tableCopy) for s in vTable}\n",
    "\n",
    "    return vTable\n",
    "\n",
    "\n",
    "def GPI(policy: dict, mdp: MDP, maxIterations: int = 10, vSweeps: int = 100) -> dict:\n",
    "    vTable = {s: 0 for s in mdp.statesPlus}\n",
    "\n",
    "    for _ in range(maxIterations):\n",
    "        # Evaluation\n",
    "        vTable = UpdateV(vTable, policy, mdp, sweeps=vSweeps)\n",
    "        actionValues = {s: UpdateQ(s, policy, mdp, vTable) for s in mdp.states}\n",
    "\n",
    "        # Improvement\n",
    "        newPolicy = UpdatePolicy(policy, actionValues)\n",
    "        \n",
    "        if newPolicy == policy:  # No more changes, hence the optimal policy has been found.\n",
    "            print('CONVERGED')\n",
    "            print('V:')\n",
    "            pprint({s: round(vTable[s], 3) for s in mdp.states})\n",
    "            print('\\nQ:')\n",
    "            pprint({s: {a: round(v, 3) for a, v in actionValues[s].items()} for s in mdp.states})\n",
    "            \n",
    "            return policy\n",
    "        \n",
    "        policy = newPolicy\n",
    "\n",
    "    print('N RUNS')\n",
    "    print('V:')\n",
    "    pprint({s: round(vTable[s], 3) for s in mdp.states})\n",
    "    print('\\nQ:')\n",
    "    pprint({s: {a: round(v, 3) for a, v in actionValues[s].items()} for s in mdp.states})\n",
    "    return policy\n",
    "\n",
    "\n",
    "def GenerateRandomPolicy(mdp: MDP) -> dict:\n",
    "    return {s: {a: 1 / len(actions) for a in actions}\n",
    "            for s, actions in ActionsFromTransitions(mdp.transitions).items()}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T12:52:37.112024200Z",
     "start_time": "2024-06-18T12:52:37.059024200Z"
    }
   },
   "id": "cfd549e125bd1d03"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "mdp = GenerateMDP(maxLevel=5,\n",
    "                  attackedChance=0,\n",
    "                  costOfLiving=0,\n",
    "                  startingState=State(level=1, tired=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T12:52:37.114021700Z",
     "start_time": "2024-06-18T12:52:37.066024700Z"
    }
   },
   "id": "7d3e753236925017"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82a1d8c4cefff03f"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "{L1 (T): {'attack': 0.5, 'rest': 0.5},\n L1 (R): {'attack': 0.3333333333333333,\n  'defend': 0.3333333333333333,\n  'train': 0.3333333333333333},\n L2 (T): {'attack': 0.5, 'rest': 0.5},\n L2 (R): {'attack': 0.3333333333333333,\n  'defend': 0.3333333333333333,\n  'train': 0.3333333333333333},\n L3 (T): {'attack': 0.5, 'rest': 0.5},\n L3 (R): {'attack': 0.3333333333333333,\n  'defend': 0.3333333333333333,\n  'train': 0.3333333333333333},\n L4 (T): {'attack': 0.5, 'rest': 0.5},\n L4 (R): {'attack': 0.3333333333333333,\n  'defend': 0.3333333333333333,\n  'train': 0.3333333333333333},\n L5 (T): {'attack': 0.5, 'rest': 0.5},\n L5 (R): {'attack': 0.5, 'defend': 0.5}}"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateRandomPolicy(mdp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T12:52:37.115022100Z",
     "start_time": "2024-06-18T12:52:37.084025400Z"
    }
   },
   "id": "66dd3f588cf7ec70"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N RUNS\n",
      "V:\n",
      "{L1 (R): -0.267,\n",
      " L1 (T): -0.45,\n",
      " L2 (R): -0.133,\n",
      " L2 (T): -0.35,\n",
      " L4 (R): 0.133,\n",
      " L5 (T): -0.05,\n",
      " L5 (R): 0.4,\n",
      " L4 (T): -0.15,\n",
      " L3 (R): 0.0,\n",
      " L3 (T): -0.25}\n",
      "\n",
      "Q:\n",
      "{L1 (R): {'attack': -0.8, 'defend': -0.253, 'train': -0.332},\n",
      " L1 (T): {'attack': -0.9, 'rest': -0.253},\n",
      " L2 (R): {'attack': -0.4, 'defend': -0.127, 'train': -0.237},\n",
      " L2 (T): {'attack': -0.7, 'rest': -0.127},\n",
      " L4 (R): {'attack': 0.4, 'defend': 0.127, 'train': -0.048},\n",
      " L5 (T): {'attack': -0.1, 'rest': 0.38},\n",
      " L5 (R): {'attack': 0.8, 'defend': 0.38},\n",
      " L4 (T): {'attack': -0.3, 'rest': 0.127},\n",
      " L3 (R): {'attack': 0.0, 'defend': 0.0, 'train': -0.142},\n",
      " L3 (T): {'attack': -0.5, 'rest': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "policy = GPI(GenerateRandomPolicy(mdp), mdp, maxIterations=1, vSweeps=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T12:52:37.116022200Z",
     "start_time": "2024-06-18T12:52:37.098032200Z"
    }
   },
   "id": "b0722bf71b6e837d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "524766578630917"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $v_{\\pi}(s)=\\sum_{a} \\pi(a|s) \\sum_{r, s'} p(r, s' | s, a) [r+ \\gamma v_{\\pi}(s')]$\n",
    "Since all values are initially set to 0, the formula can for now be simplified to:\n",
    "\n",
    "### $v_{\\pi}(s)=\\sum_{a} \\pi(a|s) \\sum_{r, s'} p(r, s' | s, a) [r]$\n",
    "\n",
    "| State  | Initial Value | Options                                             | Calculation                                                       | New Value |\n",
    "|--------|---------------|-----------------------------------------------------|-------------------------------------------------------------------|-----------|\n",
    "| L1 (T) | 0             | Attack (Died, Won), Rest (L1 (R))                   | $\\frac{1}{2}(0.95(-1) + 0.05(1)) + \\frac{1}{2}(0)$                | -0.45     |\n",
    "| L1 (R) | 0             | Attack (Died, Won), Train (L2 (T)), Defend (L1 (R)) | $\\frac{1}{3}(0.9(-1) + 0.1(1)) + \\frac{1}{3}(0) + \\frac{1}{3}(0)$ | -0.267    |\n",
    "| L2 (T) | 0             | Attack (Died, Won), Rest (L2 (R))                   | $\\frac{1}{2}(0.85(-1) + 0.15(1)) + \\frac{1}{2}(0)$                | -0.35     |\n",
    "| L2 (R) | 0             | Attack (Died, Won), Train (L3 (T)), Defend (L2 (R)) | $\\frac{1}{3}(0.7(-1) + 0.3(1)) + \\frac{1}{3}(0) + \\frac{1}{3}(0)$ | -0.133    |\n",
    "| L3 (T) | 0             | Attack (Died, Won), Rest (L3 (R))                   | $\\frac{1}{2}(0.75(-1) + 0.25(1)) + \\frac{1}{2}(0)$                | -0.25     |\n",
    "| L3 (R) | 0             | Attack (Died, Won), Train (L4 (T)), Defend (L3 (R)) | $\\frac{1}{3}(0.5(-1) + 0.5(1)) + \\frac{1}{3}(0) + \\frac{1}{3}(0)$ | 0         |\n",
    "| L4 (T) | 0             | Attack (Died, Won), Rest (L4 (R))                   | $\\frac{1}{2}(0.65(-1) + 0.35(1)) + \\frac{1}{2}(0)$                | -0.15     |\n",
    "| L4 (R) | 0             | Attack (Died, Won), Train (L5 (T)), Defend (L4 (R)) | $\\frac{1}{3}(0.3(-1) + 0.7(1)) + \\frac{1}{3}(0) + \\frac{1}{3}(0)$ | 0.133     |\n",
    "| L5 (T) | 0             | Attack (Died, Won), Rest (L5 (R))                   | $\\frac{1}{2}(0.55(-1) + 0.25(1)) + \\frac{1}{2}(0)$                | -0.05     |\n",
    "| L5 (R) | 0             | Attack (Died, Won), Defend (L5 (R))                 | $\\frac{1}{2}(0.1(-1) + 0.9(1)) + \\frac{1}{2}(0)$                  | 0.4       |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d2b7308336f2ddb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a) [r+\\gamma \\sum_{a'} \\pi (a'|s') q_{\\pi}(s', a')]$\n",
    "\n",
    "\n",
    "| State  | Action | Outcomes  | Calculation        | New Value |\n",
    "|--------|--------|-----------|--------------------|-----------|\n",
    "| L1 (T) | Attack | Died, Won | $0.95(-1)+0.05(1)$ | -0.9      |\n",
    "| L1 (T) | Rest   | L1 (R)    | $1(-0.267)$        | -0.267    |\n",
    "| L1 (R) | Attack | Died, Won | $0.9(-1)+0.1(1)$   | -0.8      |\n",
    "| L1 (R) | Train  | L2 (T)    | $1(-0.35)$         | -0.35     |\n",
    "| L1 (R) | Defend | L1 (R)    | $1(-0.267)$        | -0.35     |\n",
    "| L2 (T) | Attack | Died, Won | $0.85(-1)+0.15(1)$ | -0.7      |\n",
    "| L2 (T) | Rest   | L2 (R)    | $1(-0.133)$        | -0.133    |\n",
    "| L2 (R) | Attack | Died, Won | $0.7(-1)+0.3(1)$   | -0.4      |\n",
    "| L2 (R) | Train  | L3 (T)    | $1(-0.25)$         | -0.25     |\n",
    "| L2 (R) | Defend | L2 (R)    | $1(-0.133)$        | -0.133    |\n",
    "| L3 (T) | Attack | Died, Won | $0.75(-1)+0.25(1)$ | -0.5      |\n",
    "| L3 (T) | Rest   | L3 (R)    | $1(0)$             | 0         |\n",
    "| L3 (R) | Attack | Died, Won | $0.5(-1)+0.5(1)$   | 0         |\n",
    "| L3 (R) | Train  | L4 (T)    | $1(-0.15)$         | -0.15     |\n",
    "| L3 (R) | Defend | L3 (R)    | $1(0)$             | 0         |\n",
    "| L4 (T) | Attack | Died, Won | $0.65(-1)+0.35(1)$ | -0.3      |\n",
    "| L4 (T) | Rest   | L4 (R)    | $1(0.133)$         | 0.133     |\n",
    "| L4 (R) | Attack | Died, Won | $0.3(-1)+0.7(1)$   | 0.4       |\n",
    "| L4 (R) | Train  | L5 (T)    | $1(-0.05)$         | -0.05     |\n",
    "| L4 (R) | Defend | L4 (R)    | $1(0.133)$         | 0.133     |\n",
    "| L5 (T) | Attack | Died, Won | $0.55(-1)+0.45(1)$ | -0.1      |\n",
    "| L5 (T) | Rest   | L5 (R)    | $1(0.4)$           | 0.4       |\n",
    "| L5 (R) | Attack | Died, Won | $0.1(-1)+0.9(1)$   | 0.8       |\n",
    "| L5 (R) | Defend | L5 (R)    | $1(0.4)$           | 0.4       |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10204143f1e613bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Last pass\n",
    "\n",
    "After the policy is converged, it is no longer expected to update. Following that logic, using the last policy, state and action values to run one more iteration of GPI should result in no changes to the policy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d502347bdaad336"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGED\n",
      "V:\n",
      "{L1 (R): 0.531,\n",
      " L1 (T): 0.504,\n",
      " L2 (R): 0.588,\n",
      " L2 (T): 0.559,\n",
      " L4 (R): 0.722,\n",
      " L5 (T): 0.76,\n",
      " L5 (R): 0.8,\n",
      " L4 (T): 0.686,\n",
      " L3 (R): 0.652,\n",
      " L3 (T): 0.619}\n",
      "\n",
      "Q:\n",
      "{L1 (R): {'attack': -0.8, 'defend': 0.504, 'train': 0.531},\n",
      " L1 (T): {'attack': -0.9, 'rest': 0.504},\n",
      " L2 (R): {'attack': -0.4, 'defend': 0.559, 'train': 0.588},\n",
      " L2 (T): {'attack': -0.7, 'rest': 0.559},\n",
      " L4 (R): {'attack': 0.4, 'defend': 0.686, 'train': 0.722},\n",
      " L5 (T): {'attack': -0.1, 'rest': 0.76},\n",
      " L5 (R): {'attack': 0.8, 'defend': 0.76},\n",
      " L4 (T): {'attack': -0.3, 'rest': 0.686},\n",
      " L3 (R): {'attack': 0.0, 'defend': 0.619, 'train': 0.652},\n",
      " L3 (T): {'attack': -0.5, 'rest': 0.619}}\n"
     ]
    },
    {
     "data": {
      "text/plain": "{L1 (T): {'attack': 0, 'rest': 1},\n L1 (R): {'attack': 0, 'defend': 0, 'train': 1},\n L2 (T): {'attack': 0, 'rest': 1},\n L2 (R): {'attack': 0, 'defend': 0, 'train': 1},\n L3 (T): {'attack': 0, 'rest': 1},\n L3 (R): {'attack': 0, 'defend': 0, 'train': 1},\n L4 (T): {'attack': 0, 'rest': 1},\n L4 (R): {'attack': 0, 'defend': 0, 'train': 1},\n L5 (T): {'attack': 0, 'rest': 1},\n L5 (R): {'attack': 1, 'defend': 0}}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 0.95\n",
    "\n",
    "GPI(GenerateRandomPolicy(mdp), mdp, maxIterations=1000, vSweeps=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T12:52:37.189021500Z",
     "start_time": "2024-06-18T12:52:37.112024200Z"
    }
   },
   "id": "505f41569ebe0c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $v_{\\pi}(s)=\\sum_{a} \\pi(a|s) \\sum_{r, s'} p(r, s' | s, a) [r+ \\gamma v_{\\pi}(s')]$\n",
    "Seeing as the policy is greedy, we only need to check the value of the only action taken at state $s$.\n",
    "### $v_{\\pi}(s)=\\sum_{r, s'} p(r, s' | s, a) [r+ \\gamma v_{\\pi}(s')] \\text{ where } a=\\pi(s)$\n",
    "\n",
    "| State  | Old Value | $\\pi(s)$           | Calculation          | New Value |\n",
    "|--------|-----------|--------------------|----------------------|-----------|\n",
    "| L1 (T) | 0.504     | Rest (L1 (R))      | $0+0.95(0.504)$      | 0.504     |\n",
    "| L1 (R) | 0.531     | Train (L2 (T))     | $0+0.95(0.559)$      | 0.531     |\n",
    "| L2 (T) | 0.559     | Rest (L2 (R))      | $0+0.95(0.588)$      | 0.559     |\n",
    "| L2 (R) | 0.588     | Train (L3 (T))     | $0+0.95(0.619)$      | 0.588     |\n",
    "| L3 (T) | 0.619     | Rest (L3 (R))      | $0+0.95(0.652)$      | 0.619     |\n",
    "| L3 (R) | 0.652     | Train (L4 (T))     | $0+0.95(0.686)$      | 0.652     |\n",
    "| L4 (T) | 0.686     | Rest (L4 (R))      | $0+0.95(0.722)$      | 0.686     |\n",
    "| L4 (R) | 0.722     | Train (L5 (T))     | $0+0.95(0.76)$       | 0.722     |\n",
    "| L5 (T) | 0.76      | Rest (L5 (R))      | $0+0.95(0.8)$        | 0.76      |\n",
    "| L5 (R) | 0.8       | Attack (Died, Won) | $(0.1(-1) + 0.9(1))$ | 0.8       |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31a5d5fa5c7619d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the state values did not meaningfully change at all (chances, where present, are smaller than 0.001 and the hierarchy of state value is the same), we can be sure that the action values won't change, ergo the policy will stay the same. All this means that the code converged the policy properly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e604f52e7247b83"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
